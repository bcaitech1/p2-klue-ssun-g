{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350708b8-ac67-48db-ba1f-dd7d1ed195f2",
   "metadata": {},
   "source": [
    "## storagy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c473ca36-fbf4-4050-8d33-13f94d49d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay          99G   62G   32G  67% /\n",
      "tmpfs            64M     0   64M   0% /dev\n",
      "tmpfs            89G     0   89G   0% /sys/fs/cgroup\n",
      "shm             1.0G     0  1.0G   0% /dev/shm\n",
      "/dev/xvdb1       99G   62G   32G  67% /etc/hosts\n",
      "tmpfs            89G   12K   89G   1% /proc/driver/nvidia\n",
      "/dev/xvda1       48G  8.8G   37G  20% /usr/bin/nvidia-smi\n",
      "udev             89G     0   89G   0% /dev/nvidia0\n",
      "tmpfs            89G     0   89G   0% /proc/acpi\n",
      "tmpfs            89G     0   89G   0% /proc/scsi\n",
      "tmpfs            89G     0   89G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02a461c-551a-43d1-a32a-61d3a24ac68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35G\t.\n",
      "23G\t./model\n",
      "11G\t./.cache\n",
      "679M\t./kobert\n",
      "132M\t./input\n",
      "109M\t./.vscode-server\n",
      "4.3M\t./.ipython\n",
      "2.9M\t./.ipynb_checkpoints\n",
      "532K\t./.local\n",
      "84K\t./code\n",
      "32K\t./.jupyter\n",
      "16K\t./.config\n",
      "8.0K\t./prediction\n",
      "8.0K\t./.nv\n",
      "8.0K\t./.keras\n"
     ]
    }
   ],
   "source": [
    "!du -h --max-depth=1 | sort -hr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea181eb0-b3c9-4d92-a56e-8e35141e7b74",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00971362-bb46-4132-aeef-a4c066599b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gluonnlp as nlp\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('seaborn')\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636bbd4-b3cb-472e-844a-9f1c5ae6f530",
   "metadata": {},
   "source": [
    "## Fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f89ce7-6d7a-4995-9758-a1ec9ad439cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caefea07-9069-4af9-97c2-6d40c51a59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3e1be-4ecc-43a4-aff3-f86b5f1813f4",
   "metadata": {},
   "source": [
    "## Use cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa52809f-7e87-4740-9db7-6b4c895fac60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3060b4a-5b77-444a-9006-e8eb51b9ab71",
   "metadata": {},
   "source": [
    "## Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63bed85f-d7d4-41c2-8ce6-4f6e0d3702c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.01\n",
    "num_epochs = 30\n",
    "max_grad_norm = 1\n",
    "learning_rate =  5e-6\n",
    "stop_count = 5\n",
    "num_folds = 15\n",
    "PATH = './model/model_state_dict_init.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37314f2f-aca0-412f-a9cb-a3cd3727e623",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2480f37e-46f4-4619-b34c-ff6a4560b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4804b715-80c2-4614-9e82-1a99c3b69c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset, label_type):\n",
    "    label = []\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    out_dataset = pd.DataFrame({'0': dataset[0], 'sentence':dataset[1],'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "    return out_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9647579-d62e-43a8-b3b3-449213b7e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "  # load label_type, classes\n",
    "  with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "    label_type = pickle.load(f)\n",
    "  # load dataset\n",
    "  dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "  # preprecessing dataset\n",
    "  dataset = preprocessing_dataset(dataset, label_type)\n",
    "  \n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "144ab237-6a2c-4069-bc1c-0c2750e16261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "        temp = ''\n",
    "        temp = e01 + '</s>' + e02\n",
    "        concat_entity.append(temp)\n",
    "    tokenized_sentences = tokenizer(\n",
    "        concat_entity,\n",
    "        list(dataset['sentence']),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation='only_second',\n",
    "        max_length=max_len,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ef24ad5-5521-4a8e-ba05-1f148c6b664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data('/opt/ml/input/data/train/train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9717db9e-f54a-496d-ba6e-c8223ce5f2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8948"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10개 미만 label 삭제\n",
    "df_train = df_train[~df_train['label'].isin((40, 37, 29, 41, 19, 18, 26, 28, 39))]\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343302ad-ff45-47c5-b318-8c87a09ed4ba",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2be289-a233-4f6c-b1e5-cb19818a97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc5aaf4-7f49-4351-9bc3-3bcf3dc204c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=42, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cd3cf7f-68c3-4b19-8ec2-a3f564c7e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, XLMRobertaConfig\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-large\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cf7e149-c11e-42df-82b6-1fa9306b86c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_config = XLMRobertaConfig.from_pretrained(MODEL_NAME)\n",
    "model_config.num_labels = 42\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcbbd594-5f62-41c1-81b4-babe1acce1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 (fold마다 모델을 불러오기 위해)\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02df3253-edc5-4b0b-aad9-6e4bfa87e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = tokenized_dataset(df_train, tokenizer)\n",
    "t_label = df_train['label'].values\n",
    "dataset = RE_Dataset(t_dataset, t_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04e4c4-5cd4-4dfb-9b84-5e4896ffecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "==========\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.9435 Acc: 0.4328\n",
      "val Loss: 1.6708 Acc: 0.4964\n",
      "epochs_val acc: 0.4964\n",
      "epochs_before_best acc: 0.0000\n",
      "epochs_after_best acc: 0.4964\n",
      "Training complete in 3.0m 57.87691640853882s\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.8246 Acc: 0.5211\n",
      "val Loss: 1.4029 Acc: 0.5910\n",
      "epochs_val acc: 0.5910\n",
      "epochs_before_best acc: 0.4964\n",
      "epochs_after_best acc: 0.5910\n",
      "Training complete in 3.0m 57.997538328170776s\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.1838 Acc: 0.6568\n",
      "val Loss: 1.2788 Acc: 0.6823\n",
      "epochs_val acc: 0.6823\n",
      "epochs_before_best acc: 0.5910\n",
      "epochs_after_best acc: 0.6823\n",
      "Training complete in 3.0m 56.85075640678406s\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.2389 Acc: 0.7234\n",
      "val Loss: 0.7341 Acc: 0.7094\n",
      "epochs_val acc: 0.7094\n",
      "epochs_before_best acc: 0.6823\n",
      "epochs_after_best acc: 0.7094\n",
      "Training complete in 3.0m 57.142228841781616s\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.5550 Acc: 0.7827\n",
      "val Loss: 0.7093 Acc: 0.7383\n",
      "epochs_val acc: 0.7383\n",
      "epochs_before_best acc: 0.7094\n",
      "epochs_after_best acc: 0.7383\n",
      "Training complete in 3.0m 56.516594886779785s\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.6131 Acc: 0.8091\n",
      "val Loss: 0.9665 Acc: 0.7341\n",
      "Training complete in 3.0m 37.5020387172699s\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.7962 Acc: 0.8386\n",
      "val Loss: 0.4072 Acc: 0.7359\n",
      "Training complete in 3.0m 33.80168056488037s\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.6090 Acc: 0.8605\n",
      "val Loss: 1.2393 Acc: 0.7391\n",
      "epochs_val acc: 0.7391\n",
      "epochs_before_best acc: 0.7383\n",
      "epochs_after_best acc: 0.7391\n",
      "Training complete in 3.0m 52.44962525367737s\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.1907 Acc: 0.8806\n",
      "val Loss: 0.7848 Acc: 0.7729\n",
      "epochs_val acc: 0.7729\n",
      "epochs_before_best acc: 0.7391\n",
      "epochs_after_best acc: 0.7729\n",
      "Training complete in 3.0m 52.05300688743591s\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.4268 Acc: 0.8958\n",
      "val Loss: 0.5488 Acc: 0.7589\n",
      "Training complete in 3.0m 33.286391735076904s\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.2821 Acc: 0.9139\n",
      "val Loss: 1.0307 Acc: 0.7638\n",
      "Training complete in 3.0m 33.553908824920654s\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.9284\n",
      "val Loss: 0.6268 Acc: 0.7630\n",
      "Training complete in 3.0m 33.65857291221619s\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.3806 Acc: 0.9416\n",
      "val Loss: 0.9922 Acc: 0.7721\n",
      "Training complete in 3.0m 33.305773973464966s\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.0965 Acc: 0.9485\n",
      "val Loss: 0.9347 Acc: 0.7762\n",
      "epochs_val acc: 0.7762\n",
      "epochs_before_best acc: 0.7729\n",
      "epochs_after_best acc: 0.7762\n",
      "Training complete in 3.0m 52.35533952713013s\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.1792 Acc: 0.9557\n",
      "val Loss: 1.3954 Acc: 0.7720\n",
      "Training complete in 3.0m 33.60878872871399s\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.0829 Acc: 0.9600\n",
      "val Loss: 0.5276 Acc: 0.7704\n",
      "Training complete in 3.0m 33.64028882980347s\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.1170 Acc: 0.9674\n",
      "val Loss: 1.8056 Acc: 0.7736\n",
      "Training complete in 3.0m 33.66442942619324s\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.0554 Acc: 0.9688\n",
      "val Loss: 1.3973 Acc: 0.7810\n",
      "epochs_val acc: 0.7810\n",
      "epochs_before_best acc: 0.7762\n",
      "epochs_after_best acc: 0.7810\n",
      "Training complete in 3.0m 52.207510471343994s\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.2098 Acc: 0.9747\n",
      "val Loss: 1.0612 Acc: 0.7852\n",
      "epochs_val acc: 0.7852\n",
      "epochs_before_best acc: 0.7810\n",
      "epochs_after_best acc: 0.7852\n",
      "Training complete in 3.0m 51.76542520523071s\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.2105 Acc: 0.9802\n",
      "val Loss: 0.4998 Acc: 0.7943\n",
      "epochs_val acc: 0.7943\n",
      "epochs_before_best acc: 0.7852\n",
      "epochs_after_best acc: 0.7943\n",
      "Training complete in 3.0m 52.239586353302s\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.0460 Acc: 0.9790\n",
      "val Loss: 1.9408 Acc: 0.7909\n",
      "Training complete in 3.0m 33.241352558135986s\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.0256 Acc: 0.9801\n",
      "val Loss: 1.0585 Acc: 0.7811\n",
      "Training complete in 3.0m 33.50069069862366s\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.1816 Acc: 0.9842\n",
      "val Loss: 1.7378 Acc: 0.7737\n",
      "Training complete in 3.0m 33.50394105911255s\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.0326 Acc: 0.9860\n",
      "val Loss: 1.3080 Acc: 0.7762\n",
      "Training complete in 3.0m 32.898518800735474s\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.0250 Acc: 0.9858\n",
      "val Loss: 0.6062 Acc: 0.7812\n",
      "Training complete in 3.0m 32.92218208312988s\n",
      "...Early Stopping...\n",
      "\n",
      "Best val Acc: 0.7943295739348372\n",
      "\n",
      "FOLD 1\n",
      "==========\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 2.2865 Acc: 0.4305\n",
      "val Loss: 2.3644 Acc: 0.4930\n",
      "epochs_val acc: 0.4930\n",
      "epochs_before_best acc: 0.0000\n",
      "epochs_after_best acc: 0.4930\n",
      "Training complete in 3.0m 51.418604135513306s\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.9195 Acc: 0.5385\n",
      "val Loss: 1.2123 Acc: 0.6503\n",
      "epochs_val acc: 0.6503\n",
      "epochs_before_best acc: 0.4930\n",
      "epochs_after_best acc: 0.6503\n",
      "Training complete in 3.0m 51.77335524559021s\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.0614 Acc: 0.6779\n",
      "val Loss: 0.6380 Acc: 0.7284\n",
      "epochs_val acc: 0.7284\n",
      "epochs_before_best acc: 0.6503\n",
      "epochs_after_best acc: 0.7284\n",
      "Training complete in 3.0m 51.46433901786804s\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9548 Acc: 0.7519\n",
      "val Loss: 0.9265 Acc: 0.7433\n",
      "epochs_val acc: 0.7433\n",
      "epochs_before_best acc: 0.7284\n",
      "epochs_after_best acc: 0.7433\n",
      "Training complete in 3.0m 51.608466148376465s\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.6079 Acc: 0.7975\n",
      "val Loss: 0.7376 Acc: 0.7448\n",
      "epochs_val acc: 0.7448\n",
      "epochs_before_best acc: 0.7433\n",
      "epochs_after_best acc: 0.7448\n",
      "Training complete in 3.0m 51.581538677215576s\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.7175 Acc: 0.8301\n",
      "val Loss: 1.6969 Acc: 0.7496\n",
      "epochs_val acc: 0.7496\n",
      "epochs_before_best acc: 0.7448\n",
      "epochs_after_best acc: 0.7496\n",
      "Training complete in 3.0m 51.66720223426819s\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.4970 Acc: 0.8583\n",
      "val Loss: 0.7392 Acc: 0.7490\n",
      "Training complete in 3.0m 33.1165931224823s\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.5444 Acc: 0.8789\n",
      "val Loss: 0.4427 Acc: 0.7491\n",
      "Training complete in 3.0m 32.74282121658325s\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.6702 Acc: 0.8988\n",
      "val Loss: 1.0955 Acc: 0.7523\n",
      "epochs_val acc: 0.7523\n",
      "epochs_before_best acc: 0.7496\n",
      "epochs_after_best acc: 0.7523\n",
      "Training complete in 3.0m 51.66713857650757s\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.2487 Acc: 0.9125\n",
      "val Loss: 1.0665 Acc: 0.7605\n",
      "epochs_val acc: 0.7605\n",
      "epochs_before_best acc: 0.7523\n",
      "epochs_after_best acc: 0.7605\n",
      "Training complete in 3.0m 52.1678032875061s\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.1380 Acc: 0.9193\n",
      "val Loss: 1.0379 Acc: 0.7622\n",
      "epochs_val acc: 0.7622\n",
      "epochs_before_best acc: 0.7605\n",
      "epochs_after_best acc: 0.7622\n",
      "Training complete in 3.0m 51.569642543792725s\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.0945 Acc: 0.9383\n",
      "val Loss: 1.3357 Acc: 0.7613\n",
      "Training complete in 3.0m 32.70599055290222s\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.1419 Acc: 0.9507\n",
      "val Loss: 0.6722 Acc: 0.7762\n",
      "epochs_val acc: 0.7762\n",
      "epochs_before_best acc: 0.7622\n",
      "epochs_after_best acc: 0.7762\n",
      "Training complete in 3.0m 51.42729663848877s\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.4364 Acc: 0.9578\n",
      "val Loss: 1.4396 Acc: 0.7744\n",
      "Training complete in 3.0m 32.71620750427246s\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.1068 Acc: 0.9620\n",
      "val Loss: 1.7086 Acc: 0.7761\n",
      "Training complete in 3.0m 33.1709942817688s\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.0543 Acc: 0.9658\n",
      "val Loss: 1.3349 Acc: 0.7794\n",
      "epochs_val acc: 0.7794\n",
      "epochs_before_best acc: 0.7762\n",
      "epochs_after_best acc: 0.7794\n",
      "Training complete in 3.0m 51.39906144142151s\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.0600 Acc: 0.9710\n",
      "val Loss: 0.9344 Acc: 0.7803\n",
      "epochs_val acc: 0.7803\n",
      "epochs_before_best acc: 0.7794\n",
      "epochs_after_best acc: 0.7803\n",
      "Training complete in 3.0m 51.8145694732666s\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.1460 Acc: 0.9754\n",
      "val Loss: 0.3859 Acc: 0.7828\n",
      "epochs_val acc: 0.7828\n",
      "epochs_before_best acc: 0.7803\n",
      "epochs_after_best acc: 0.7828\n",
      "Training complete in 3.0m 51.53335189819336s\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.0308 Acc: 0.9775\n",
      "val Loss: 0.8362 Acc: 0.7918\n",
      "epochs_val acc: 0.7918\n",
      "epochs_before_best acc: 0.7828\n",
      "epochs_after_best acc: 0.7918\n",
      "Training complete in 3.0m 51.64603900909424s\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.1302 Acc: 0.9794\n",
      "val Loss: 1.2462 Acc: 0.7835\n",
      "Training complete in 3.0m 33.03758144378662s\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.0178 Acc: 0.9824\n",
      "val Loss: 0.0258 Acc: 0.7812\n",
      "Training complete in 3.0m 32.7740592956543s\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.0143 Acc: 0.9855\n",
      "val Loss: 2.1243 Acc: 0.7794\n",
      "Training complete in 3.0m 32.47400116920471s\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.0931 Acc: 0.9844\n",
      "val Loss: 1.4615 Acc: 0.7934\n",
      "epochs_val acc: 0.7934\n",
      "epochs_before_best acc: 0.7918\n",
      "epochs_after_best acc: 0.7934\n",
      "Training complete in 3.0m 51.85912823677063s\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.2376 Acc: 0.9869\n",
      "val Loss: 1.0842 Acc: 0.7860\n",
      "Training complete in 3.0m 32.740076303482056s\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.0352 Acc: 0.9874\n",
      "val Loss: 2.1324 Acc: 0.7868\n",
      "Training complete in 3.0m 32.390910148620605s\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 0.9875\n",
      "val Loss: 0.5919 Acc: 0.7951\n",
      "epochs_val acc: 0.7951\n",
      "epochs_before_best acc: 0.7934\n",
      "epochs_after_best acc: 0.7951\n",
      "Training complete in 3.0m 51.178881883621216s\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.0135 Acc: 0.9899\n",
      "val Loss: 1.2409 Acc: 0.7893\n",
      "Training complete in 3.0m 32.71307897567749s\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.0210 Acc: 0.9899\n",
      "val Loss: 1.4426 Acc: 0.7852\n",
      "Training complete in 3.0m 32.25679326057434s\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.0682 Acc: 0.9899\n",
      "val Loss: 2.0253 Acc: 0.7859\n",
      "Training complete in 3.0m 32.43214201927185s\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.0433 Acc: 0.9898\n",
      "val Loss: 1.3233 Acc: 0.7859\n",
      "Training complete in 3.0m 32.67875075340271s\n",
      "\n",
      "Best val Acc: 0.7951127819548872\n",
      "\n",
      "FOLD 2\n",
      "==========\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.7627 Acc: 0.4282\n",
      "val Loss: 2.9165 Acc: 0.4904\n",
      "epochs_val acc: 0.4904\n",
      "epochs_before_best acc: 0.0000\n",
      "epochs_after_best acc: 0.4904\n",
      "Training complete in 3.0m 51.48643469810486s\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.8705 Acc: 0.5256\n",
      "val Loss: 1.4965 Acc: 0.6017\n",
      "epochs_val acc: 0.6017\n",
      "epochs_before_best acc: 0.4904\n",
      "epochs_after_best acc: 0.6017\n",
      "Training complete in 3.0m 51.418028354644775s\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.6292 Acc: 0.6599\n",
      "val Loss: 0.5292 Acc: 0.7244\n",
      "epochs_val acc: 0.7244\n",
      "epochs_before_best acc: 0.6017\n",
      "epochs_after_best acc: 0.7244\n",
      "Training complete in 3.0m 51.5476438999176s\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.5521 Acc: 0.7309\n",
      "val Loss: 0.4847 Acc: 0.7195\n",
      "Training complete in 3.0m 32.67428231239319s\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.5182 Acc: 0.7818\n",
      "val Loss: 0.6312 Acc: 0.7506\n",
      "epochs_val acc: 0.7506\n",
      "epochs_before_best acc: 0.7244\n",
      "epochs_after_best acc: 0.7506\n",
      "Training complete in 3.0m 51.4035222530365s\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.6727 Acc: 0.8213\n",
      "val Loss: 0.7694 Acc: 0.7490\n",
      "Training complete in 3.0m 32.71423602104187s\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.2395 Acc: 0.8439\n",
      "val Loss: 0.2318 Acc: 0.7408\n",
      "Training complete in 3.0m 32.959203481674194s\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.6256 Acc: 0.8686\n",
      "val Loss: 0.8515 Acc: 0.7424\n",
      "Training complete in 3.0m 32.85734248161316s\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.5038 Acc: 0.8866\n",
      "val Loss: 0.7018 Acc: 0.7515\n",
      "epochs_val acc: 0.7515\n",
      "epochs_before_best acc: 0.7506\n",
      "epochs_after_best acc: 0.7515\n",
      "Training complete in 3.0m 51.42288947105408s\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.4204 Acc: 0.9055\n",
      "val Loss: 0.7230 Acc: 0.7687\n",
      "epochs_val acc: 0.7687\n",
      "epochs_before_best acc: 0.7515\n",
      "epochs_after_best acc: 0.7687\n",
      "Training complete in 3.0m 51.97372317314148s\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.1734 Acc: 0.9210\n",
      "val Loss: 1.2139 Acc: 0.7612\n",
      "Training complete in 3.0m 32.62853384017944s\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.1314 Acc: 0.9355\n",
      "val Loss: 1.0066 Acc: 0.7778\n",
      "epochs_val acc: 0.7778\n",
      "epochs_before_best acc: 0.7687\n",
      "epochs_after_best acc: 0.7778\n",
      "Training complete in 3.0m 51.703099727630615s\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.2223 Acc: 0.9416\n",
      "val Loss: 0.4172 Acc: 0.7688\n",
      "Training complete in 3.0m 33.2318811416626s\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.1114 Acc: 0.9514\n",
      "val Loss: 1.0410 Acc: 0.7720\n",
      "Training complete in 3.0m 33.33195090293884s\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.4603 Acc: 0.9587\n",
      "val Loss: 0.6287 Acc: 0.7547\n",
      "Training complete in 3.0m 32.83372688293457s\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.1622 Acc: 0.9637\n",
      "val Loss: 1.6921 Acc: 0.7604\n",
      "Training complete in 3.0m 33.036689043045044s\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.0276 Acc: 0.9703\n",
      "val Loss: 0.4097 Acc: 0.7746\n",
      "Training complete in 3.0m 33.198877811431885s\n",
      "...Early Stopping...\n",
      "\n",
      "Best val Acc: 0.7778038847117795\n",
      "\n",
      "FOLD 3\n",
      "==========\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 2.2468 Acc: 0.4269\n",
      "val Loss: 2.3172 Acc: 0.4938\n",
      "epochs_val acc: 0.4938\n",
      "epochs_before_best acc: 0.0000\n",
      "epochs_after_best acc: 0.4938\n",
      "Training complete in 3.0m 51.619425773620605s\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.7662 Acc: 0.5247\n",
      "val Loss: 2.1231 Acc: 0.5892\n",
      "epochs_val acc: 0.5892\n",
      "epochs_before_best acc: 0.4938\n",
      "epochs_after_best acc: 0.5892\n",
      "Training complete in 3.0m 51.63482737541199s\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 0.9523 Acc: 0.6672\n",
      "val Loss: 0.8929 Acc: 0.7086\n",
      "epochs_val acc: 0.7086\n",
      "epochs_before_best acc: 0.5892\n",
      "epochs_after_best acc: 0.7086\n",
      "Training complete in 3.0m 51.63665199279785s\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.4625 Acc: 0.7453\n",
      "val Loss: 0.7449 Acc: 0.7614\n",
      "epochs_val acc: 0.7614\n",
      "epochs_before_best acc: 0.7086\n",
      "epochs_after_best acc: 0.7614\n",
      "Training complete in 3.0m 51.78210783004761s\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.8319 Acc: 0.7967\n",
      "val Loss: 0.6295 Acc: 0.7596\n",
      "Training complete in 3.0m 32.82355785369873s\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.2331 Acc: 0.8278\n",
      "val Loss: 0.5854 Acc: 0.7737\n",
      "epochs_val acc: 0.7737\n",
      "epochs_before_best acc: 0.7614\n",
      "epochs_after_best acc: 0.7737\n",
      "Training complete in 3.0m 51.932748794555664s\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.5924 Acc: 0.8546\n",
      "val Loss: 0.8333 Acc: 0.7433\n",
      "Training complete in 3.0m 33.3705370426178s\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.2004 Acc: 0.8783\n",
      "val Loss: 1.0082 Acc: 0.7556\n",
      "Training complete in 3.0m 32.97760033607483s\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.3288 Acc: 0.9012\n",
      "val Loss: 0.8512 Acc: 0.7762\n",
      "epochs_val acc: 0.7762\n",
      "epochs_before_best acc: 0.7737\n",
      "epochs_after_best acc: 0.7762\n",
      "Training complete in 3.0m 51.69462823867798s\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.1288 Acc: 0.9168\n",
      "val Loss: 0.9209 Acc: 0.7811\n",
      "epochs_val acc: 0.7811\n",
      "epochs_before_best acc: 0.7762\n",
      "epochs_after_best acc: 0.7811\n",
      "Training complete in 3.0m 51.345396280288696s\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.3280 Acc: 0.9302\n",
      "val Loss: 0.7641 Acc: 0.7901\n",
      "epochs_val acc: 0.7901\n",
      "epochs_before_best acc: 0.7811\n",
      "epochs_after_best acc: 0.7901\n",
      "Training complete in 3.0m 51.173052072525024s\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.2937 Acc: 0.9414\n",
      "val Loss: 1.2544 Acc: 0.7843\n",
      "Training complete in 3.0m 32.82687997817993s\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.0714 Acc: 0.9537\n",
      "val Loss: 0.9633 Acc: 0.7943\n",
      "epochs_val acc: 0.7943\n",
      "epochs_before_best acc: 0.7901\n",
      "epochs_after_best acc: 0.7943\n",
      "Training complete in 3.0m 51.59670853614807s\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.0777 Acc: 0.9607\n",
      "val Loss: 0.6980 Acc: 0.8042\n",
      "epochs_val acc: 0.8042\n",
      "epochs_before_best acc: 0.7943\n",
      "epochs_after_best acc: 0.8042\n",
      "Training complete in 3.0m 51.6564154624939s\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.0561 Acc: 0.9638\n",
      "val Loss: 1.6293 Acc: 0.7794\n",
      "Training complete in 3.0m 32.844181537628174s\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.2711 Acc: 0.9716\n",
      "val Loss: 0.6885 Acc: 0.7910\n",
      "Training complete in 3.0m 32.83282685279846s\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.0984 Acc: 0.9758\n",
      "val Loss: 1.1015 Acc: 0.7885\n",
      "Training complete in 3.0m 32.9055392742157s\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.0135 Acc: 0.9786\n",
      "val Loss: 1.2971 Acc: 0.7877\n",
      "Training complete in 3.0m 33.3037965297699s\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.0178 Acc: 0.9804\n",
      "val Loss: 1.0000 Acc: 0.7910\n",
      "Training complete in 3.0m 32.903873443603516s\n",
      "...Early Stopping...\n",
      "\n",
      "Best val Acc: 0.8041979949874687\n",
      "\n",
      "FOLD 4\n",
      "==========\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 2.1854 Acc: 0.4294\n",
      "val Loss: 1.4529 Acc: 0.4973\n",
      "epochs_val acc: 0.4973\n",
      "epochs_before_best acc: 0.0000\n",
      "epochs_after_best acc: 0.4973\n",
      "Training complete in 3.0m 52.671271085739136s\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.4268 Acc: 0.5279\n",
      "val Loss: 0.8370 Acc: 0.6396\n",
      "epochs_val acc: 0.6396\n",
      "epochs_before_best acc: 0.4973\n",
      "epochs_after_best acc: 0.6396\n",
      "Training complete in 3.0m 51.80533194541931s\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.3356 Acc: 0.6647\n",
      "val Loss: 1.8131 Acc: 0.6978\n",
      "epochs_val acc: 0.6978\n",
      "epochs_before_best acc: 0.6396\n",
      "epochs_after_best acc: 0.6978\n",
      "Training complete in 3.0m 52.174885749816895s\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.8529 Acc: 0.7424\n",
      "val Loss: 0.4655 Acc: 0.7293\n",
      "epochs_val acc: 0.7293\n",
      "epochs_before_best acc: 0.6978\n",
      "epochs_after_best acc: 0.7293\n",
      "Training complete in 3.0m 51.958343505859375s\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9962 Acc: 0.7884\n",
      "val Loss: 0.5515 Acc: 0.7688\n",
      "epochs_val acc: 0.7688\n",
      "epochs_before_best acc: 0.7293\n",
      "epochs_after_best acc: 0.7688\n",
      "Training complete in 3.0m 51.57662320137024s\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.6958 Acc: 0.8235\n",
      "val Loss: 1.1123 Acc: 0.7374\n",
      "Training complete in 3.0m 33.39096260070801s\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.8178 Acc: 0.8497\n",
      "val Loss: 1.0840 Acc: 0.7589\n",
      "Training complete in 3.0m 33.38366484642029s\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.2250 Acc: 0.8748\n",
      "val Loss: 0.8123 Acc: 0.7465\n",
      "Training complete in 3.0m 33.38659882545471s\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.3877 Acc: 0.8933\n",
      "val Loss: 1.4813 Acc: 0.7423\n",
      "Training complete in 3.0m 33.558550119400024s\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.1941 Acc: 0.9050\n",
      "val Loss: 1.4475 Acc: 0.7472\n",
      "Training complete in 3.0m 33.44914102554321s\n",
      "...Early Stopping...\n",
      "\n",
      "Best val Acc: 0.768796992481203\n",
      "\n",
      "FOLD 5\n",
      "==========\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 2.6580 Acc: 0.4300\n",
      "val Loss: 2.2577 Acc: 0.4938\n",
      "epochs_val acc: 0.4938\n",
      "epochs_before_best acc: 0.0000\n",
      "epochs_after_best acc: 0.4938\n",
      "Training complete in 3.0m 52.16444134712219s\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.2569 Acc: 0.5170\n",
      "val Loss: 1.5205 Acc: 0.6222\n",
      "epochs_val acc: 0.6222\n",
      "epochs_before_best acc: 0.4938\n",
      "epochs_after_best acc: 0.6222\n",
      "Training complete in 3.0m 52.11693263053894s\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.3621 Acc: 0.6632\n",
      "val Loss: 1.0647 Acc: 0.7005\n",
      "epochs_val acc: 0.7005\n",
      "epochs_before_best acc: 0.6222\n",
      "epochs_after_best acc: 0.7005\n",
      "Training complete in 3.0m 52.28556418418884s\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.7470 Acc: 0.7339\n",
      "val Loss: 0.7845 Acc: 0.7580\n",
      "epochs_val acc: 0.7580\n",
      "epochs_before_best acc: 0.7005\n",
      "epochs_after_best acc: 0.7580\n",
      "Training complete in 3.0m 51.86144161224365s\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.3858 Acc: 0.7869\n",
      "val Loss: 0.5668 Acc: 0.7548\n",
      "Training complete in 3.0m 33.24586606025696s\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.2171 Acc: 0.8212\n",
      "val Loss: 0.8455 Acc: 0.7704\n",
      "epochs_val acc: 0.7704\n",
      "epochs_before_best acc: 0.7580\n",
      "epochs_after_best acc: 0.7704\n",
      "Training complete in 3.0m 52.566763401031494s\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.5119 Acc: 0.8503\n",
      "val Loss: 1.2302 Acc: 0.7736\n",
      "epochs_val acc: 0.7736\n",
      "epochs_before_best acc: 0.7704\n",
      "epochs_after_best acc: 0.7736\n",
      "Training complete in 3.0m 52.14398527145386s\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.6540 Acc: 0.8727\n",
      "val Loss: 0.4240 Acc: 0.7861\n",
      "epochs_val acc: 0.7861\n",
      "epochs_before_best acc: 0.7736\n",
      "epochs_after_best acc: 0.7861\n",
      "Training complete in 3.0m 52.46599817276001s\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.2379 Acc: 0.8943\n",
      "val Loss: 0.8157 Acc: 0.7721\n",
      "Training complete in 3.0m 33.132890462875366s\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.4631 Acc: 0.9139\n",
      "val Loss: 1.0796 Acc: 0.7729\n",
      "Training complete in 3.0m 33.109193563461304s\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# kfold = KFold(n_splits=num_folds, random_state=0, shuffle=True)\n",
    "kfold = StratifiedKFold(n_splits=num_folds, random_state=0, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "results = {}\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset, dataset.labels)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('='*10)\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_subsampler,\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=val_subsampler,\n",
    "    )\n",
    "    \n",
    "    # 모델을 불러온다. (huggingface로 계속 불러오면 메모리 초과 발생!)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    \n",
    "    t_total = len(train_loader) * num_epochs\n",
    "    warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    early_stopping_count = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        train_acc = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        #################### Train ####################\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch_id, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_acc += calc_accuracy(outputs[1], labels)\n",
    "            train_loss = loss.data.cpu().numpy()\n",
    "\n",
    "        print(f\"train Loss: {train_loss:.4f} Acc: {train_acc/(batch_id+1):.4f}\")\n",
    "\n",
    "        #################### Validation ####################\n",
    "        val_loss =0.0\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id, batch in enumerate(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_acc += calc_accuracy(outputs[1], labels)\n",
    "            val_loss = outputs[0].data.cpu().numpy()\n",
    "\n",
    "        print(f\"val Loss: {val_loss:.4f} Acc: {val_acc/(batch_id+1):.4f}\")\n",
    "\n",
    "        #################### model save ####################\n",
    "        if (val_acc/(batch_id+1)) > best_acc:\n",
    "            early_stopping_count = 0\n",
    "            print(f\"epochs_val acc: {val_acc/(batch_id+1):.4f}\")\n",
    "            print(f\"epochs_before_best acc: {best_acc:.4f}\")\n",
    "            best_acc = (val_acc/(batch_id+1))\n",
    "            print(f\"epochs_after_best acc: {best_acc:.4f}\")\n",
    "            torch.save(model.state_dict(), f\"/opt/ml/model/model_state_dict{fold}.pt\")\n",
    "        \n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "        \n",
    "        #################### running time check ####################\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60}m {time_elapsed % 60}s')\n",
    "        \n",
    "        #################### Early Stopping ####################\n",
    "        if early_stopping_count >= stop_count:\n",
    "            print('...Early Stopping...')\n",
    "            print()\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    results[fold] = best_acc\n",
    "    print(f'Best val Acc: {best_acc}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c649d8-79c6-4fb1-a692-f3533b817fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {num_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    sum += value\n",
    "\n",
    "print(f'Average: {sum/len(results.items())} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675172a2-d74b-4fd9-a7b5-3177d0a25207",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefea44-6653-4175-9c85-b28a74642fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dir = \"/opt/ml/input/data/test/test.tsv\"\n",
    "test_data = load_data(test_dataset_dir)\n",
    "test_label = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d7a87-233d-4863-8cff-91198768c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenized_dataset(test_data, tokenizer)\n",
    "test_dataset = RE_Dataset(tokenized_test ,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db2012-ea7e-4836-a600-202054d685bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=25,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91519a94-102c-4d28-9570-06ff00f2c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred = None\n",
    "\n",
    "for i in range(num_folds):\n",
    "    model.load_state_dict(torch.load(f'/opt/ml/model/model_state_dict{i}.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    fold_pred = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "#                 token_type_ids=data['token_type_ids'].to(device)\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        fold_pred.append(logits)\n",
    "\n",
    "    if oof_pred is None:\n",
    "        oof_pred = np.array(fold_pred) / num_folds\n",
    "    else:\n",
    "        oof_pred += (np.array(fold_pred) / num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670b56a-d09e-47c8-9aa5-95ad858e6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace4df8-a701-450c-985d-493e48939d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(oof_pred, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d420f4-51d1-4a39-b899-bbac2fecc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = [pred[i][j] for i in range(len(pred)) for j in range(len(pred[i]))]\n",
    "submission = pd.DataFrame(submission, columns=['pred'])\n",
    "submission.to_csv('/opt/ml/prediction/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be7512-f3f0-4a27-9e40-d613d708d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4200088-4667-4a4a-aa3a-9419f21d55b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811c7f6-7b9f-48d3-a396-23089922be88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727f310-6c6f-48c3-a9b0-4c23997f75c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
