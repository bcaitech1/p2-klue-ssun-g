{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350708b8-ac67-48db-ba1f-dd7d1ed195f2",
   "metadata": {},
   "source": [
    "## storagy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c473ca36-fbf4-4050-8d33-13f94d49d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay          99G   59G   35G  63% /\n",
      "tmpfs            64M     0   64M   0% /dev\n",
      "tmpfs            89G     0   89G   0% /sys/fs/cgroup\n",
      "shm             1.0G     0  1.0G   0% /dev/shm\n",
      "/dev/xvdb1       99G   59G   35G  63% /etc/hosts\n",
      "tmpfs            89G   12K   89G   1% /proc/driver/nvidia\n",
      "/dev/xvda1       48G  8.7G   37G  20% /usr/bin/nvidia-smi\n",
      "udev             89G     0   89G   0% /dev/nvidia0\n",
      "tmpfs            89G     0   89G   0% /proc/acpi\n",
      "tmpfs            89G     0   89G   0% /proc/scsi\n",
      "tmpfs            89G     0   89G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02a461c-551a-43d1-a32a-61d3a24ac68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13G\t.\n",
      "9.3G\t./.cache\n",
      "2.1G\t./model\n",
      "679M\t./kobert\n",
      "132M\t./input\n",
      "109M\t./.vscode-server\n",
      "5.1M\t./.ipynb_checkpoints\n",
      "3.5M\t./.ipython\n",
      "444K\t./.local\n",
      "84K\t./code\n",
      "32K\t./.jupyter\n",
      "16K\t./.config\n",
      "8.0K\t./prediction\n",
      "8.0K\t./.nv\n",
      "8.0K\t./.keras\n"
     ]
    }
   ],
   "source": [
    "!du -h --max-depth=1 | sort -hr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea181eb0-b3c9-4d92-a56e-8e35141e7b74",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00971362-bb46-4132-aeef-a4c066599b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gluonnlp as nlp\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('seaborn')\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636bbd4-b3cb-472e-844a-9f1c5ae6f530",
   "metadata": {},
   "source": [
    "## Fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37746c74-784a-4355-a0f4-8f78313c02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3e1be-4ecc-43a4-aff3-f86b5f1813f4",
   "metadata": {},
   "source": [
    "## Use cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa52809f-7e87-4740-9db7-6b4c895fac60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3060b4a-5b77-444a-9006-e8eb51b9ab71",
   "metadata": {},
   "source": [
    "## Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63bed85f-d7d4-41c2-8ce6-4f6e0d3702c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.01\n",
    "num_epochs = 4\n",
    "max_grad_norm = 1\n",
    "learning_rate =  5e-4\n",
    "num_folds = 10\n",
    "PATH = './model/model_state_dict_init'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37314f2f-aca0-412f-a9cb-a3cd3727e623",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2480f37e-46f4-4619-b34c-ff6a4560b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4804b715-80c2-4614-9e82-1a99c3b69c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset, label_type):\n",
    "    label = []\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    out_dataset = pd.DataFrame({'0': dataset[0], 'sentence':dataset[1],'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "    return out_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9647579-d62e-43a8-b3b3-449213b7e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "  # load label_type, classes\n",
    "  with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "    label_type = pickle.load(f)\n",
    "  # load dataset\n",
    "  dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "  # preprecessing dataset\n",
    "  dataset = preprocessing_dataset(dataset, label_type)\n",
    "  \n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "144ab237-6a2c-4069-bc1c-0c2750e16261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "        temp = ''\n",
    "        temp = e01 + ' </s> ' + e02\n",
    "        concat_entity.append(temp)\n",
    "    tokenized_sentences = tokenizer(\n",
    "        concat_entity,\n",
    "        list(dataset['sentence']),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation='only_second',\n",
    "        max_length=128,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ef24ad5-5521-4a8e-ba05-1f148c6b664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data('/opt/ml/input/data/train/train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13cc47c4-643a-46d6-96cc-a69d3ac62b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8985"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[~df_train['label'].isin((41, 37, 40, 29))]\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343302ad-ff45-47c5-b318-8c87a09ed4ba",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a2be289-a233-4f6c-b1e5-cb19818a97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 계산\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cd3cf7f-68c3-4b19-8ec2-a3f564c7e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, XLMRobertaConfig\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-large\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687715e-f0a2-4f94-a8cc-86e70c8f0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val = train_test_split(df_train, test_size=0.15, random_state=0)\n",
    "\n",
    "# tokenized_train = tokenized_dataset(train, tokenizer)\n",
    "# tokenized_val = tokenized_dataset(val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32dcc2-02ab-406e-a917-0031cbc439f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label = train['label'].values\n",
    "# val_label = val['label'].values\n",
    "\n",
    "# train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "# val_dataset = RE_Dataset(tokenized_val, val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cf7e149-c11e-42df-82b6-1fa9306b86c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_config = XLMRobertaConfig.from_pretrained(MODEL_NAME)\n",
    "model_config.num_labels = 42\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcbbd594-5f62-41c1-81b4-babe1acce1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 (fold마다 모델을 불러오기 위해)\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02df3253-edc5-4b0b-aad9-6e4bfa87e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = tokenized_dataset(df_train, tokenizer)\n",
    "t_label = df_train['label'].values\n",
    "dataset = RE_Dataset(t_dataset, t_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04e4c4-5cd4-4dfb-9b84-5e4896ffecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "==========\n",
      "Epoch 0/3\n",
      "----------\n",
      "train Loss: 1.9572 Acc: 0.4843\n",
      "val Loss: 1.2585 Acc: 0.4831\n",
      "epochs_val acc: 0.4831\n",
      "epochs_before_best acc: 0.0000\n",
      "epochs_after_best acc: 0.4831\n",
      "Training complete in 3.0m 41.2884886264801s\n",
      "\n",
      "Epoch 1/3\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=0, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "results = {}\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('='*10)\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_subsampler\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=val_subsampler\n",
    "    )\n",
    "    \n",
    "    # 모델을 불러온다. (huggingface로 계속 불러오면 메모리 초과 발생..)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    \n",
    "    t_total = len(train_loader) * num_epochs\n",
    "    warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        train_acc = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        #################### Train ####################\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch_id, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_acc += calc_accuracy(outputs[1], labels)\n",
    "            train_loss = loss.data.cpu().numpy()\n",
    "\n",
    "        print(f\"train Loss: {train_loss:.4f} Acc: {train_acc/(batch_id+1):.4f}\")\n",
    "\n",
    "        #################### Validation ####################\n",
    "        val_loss =0.0\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id, batch in enumerate(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_acc += calc_accuracy(outputs[1], labels)\n",
    "            val_loss = outputs[0].data.cpu().numpy()\n",
    "\n",
    "        print(f\"val Loss: {val_loss:.4f} Acc: {val_acc/(batch_id+1):.4f}\")\n",
    "\n",
    "        #################### model save ####################\n",
    "        if (val_acc/(batch_id+1)) >= best_acc:\n",
    "            print(f\"epochs_val acc: {val_acc/(batch_id+1):.4f}\")\n",
    "            print(f\"epochs_before_best acc: {best_acc:.4f}\")\n",
    "            best_acc = (val_acc/(batch_id+1))\n",
    "            print(f\"epochs_after_best acc: {best_acc:.4f}\")\n",
    "            torch.save(model.state_dict(), f\"/opt/ml/model/model_state_dict{fold}.pt\")\n",
    "            \n",
    "        \n",
    "        #################### running time check ####################\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60}m {time_elapsed % 60}s')\n",
    "        print()\n",
    "    \n",
    "    results[fold] = best_acc\n",
    "    print(f'Best val Acc: {best_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c649d8-79c6-4fb1-a692-f3533b817fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {num_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    sum += value\n",
    "\n",
    "print(f'Average: {sum/len(results.items())} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675172a2-d74b-4fd9-a7b5-3177d0a25207",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefea44-6653-4175-9c85-b28a74642fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_data(r\"/opt/ml/input/data/test/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d7a87-233d-4863-8cff-91198768c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_dataset = tokenized_dataset(df_test, tokenizer)\n",
    "test_label = df_test['label'].values\n",
    "test_dataset = RE_Dataset(test_token_dataset, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db2012-ea7e-4836-a600-202054d685bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=5,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91519a94-102c-4d28-9570-06ff00f2c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred = None\n",
    "\n",
    "for i in range(num_folds):\n",
    "    model.load_state_dict(torch.load(f\"/opt/ml/model/model_state_dict{i}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, labels) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length = valid_length\n",
    "            labels = labels.long().to(device)\n",
    "            pred = model(token_ids, valid_length, segment_ids)\n",
    "#             pred = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "\n",
    "        fold_pred = np.array(all_predictions)\n",
    "        \n",
    "    if oof_pred is None:\n",
    "        oof_pred = fold_pred / num_folds\n",
    "    else:\n",
    "        oof_pred = oof_pred + (fold_pred / num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d420f4-51d1-4a39-b899-bbac2fecc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = np.argmax(oof_pred, axis=1)\n",
    "submission = pd.DataFrame(submission, columns=['pred'])\n",
    "submission.to_csv('/opt/ml/prediction/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be7512-f3f0-4a27-9e40-d613d708d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4200088-4667-4a4a-aa3a-9419f21d55b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811c7f6-7b9f-48d3-a396-23089922be88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727f310-6c6f-48c3-a9b0-4c23997f75c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
