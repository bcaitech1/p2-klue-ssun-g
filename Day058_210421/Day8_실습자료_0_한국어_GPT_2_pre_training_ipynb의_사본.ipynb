{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day8_실습자료_0_한국어_GPT_2_pre_training.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrzmQff3nU6z"
      },
      "source": [
        "# GPT-2 학습해보기\n",
        "\n",
        "> 작성자      \n",
        "```\n",
        "* 김성현 (bananaband657@gmail.com)  \n",
        "김바다 (qkek983@gmail.com)\n",
        "박상희 (parksanghee0103@gmail.com)  \n",
        "이정우 (jungwoo.l2.rs@gmail.com)\n",
        "```\n",
        "[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmoeUd5qnaq9"
      },
      "source": [
        "이번 시간엔 한국어 코퍼스를 활용해, 직접 한국어 GPT-2를 학습해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id8FcYRa48Gc",
        "outputId": "5f4df080-dcde-4294-8214-833f8da7a479"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrihNAMK5EsC",
        "outputId": "ee025c82-9eb8-45b8-a146-ca6737d07071"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbfDhMRUC-OL"
      },
      "source": [
        "역시 위키 데이터를 가져와볼까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UYtcoe0C9QT",
        "outputId": "8f95a0f5-1314-4ca6-965c-bc043601fb7b"
      },
      "source": [
        "!mkdir my_data\n",
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘my_data’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    413      0 --:--:-- --:--:-- --:--:--   413\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 1323k  100 1323k    0     0  1063k      0  0:00:01  0:00:01 --:--:-- 1063k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB-X8eCflkD-"
      },
      "source": [
        "path = \"/content/my_data/wiki_20190620_small.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAIHClHAgz5U"
      },
      "source": [
        "지금까지는 BertWordPieceTokenizer를 사용해왔다면,   \n",
        "이번에는 SentencePiceBPETokenizer를 사용해 모델을 학습해보겠습니다.\n",
        "\n",
        "각 tokenizer의 차이는 허훈님의 블로그 [여기](https://huffon.github.io/2020/07/05/tokenizers/) 에서 확인하실 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC7g-nhTu0LN"
      },
      "source": [
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from tokenizers.normalizers import BertNormalizer\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer()\n",
        "\n",
        "tokenizer._tokenizer.normalizer = BertNormalizer(clean_text=True,\n",
        "handle_chinese_chars=False,\n",
        "lowercase=False)\n",
        "\n",
        "tokenizer.train(\n",
        "    path,\n",
        "    vocab_size=10000,\n",
        "    special_tokens=[\n",
        "        \"<s>\",\n",
        "        \"<pad>\",\n",
        "        \"</s>\",\n",
        "        \"<unk>\",\n",
        "    ],\n",
        ")\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irrc7HKq8kpi",
        "outputId": "81409ff6-798d-4f3c-fc3e-464056cfecb9"
      },
      "source": [
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n",
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n",
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n",
        "print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))\n",
        "# SentencePiece를 사용하면, 나중에 decoding 과정에서 '_' 만 ' '로 replace해주면 띄어쓰기 복원이 가능해집니다."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "[1005, 579, 6613, 1303, 1041, 2071, 1136, 596, 1033]\n",
            "['▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.']\n",
            "이순신은 조선 중기의 무신이다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQyn3mpR-YiI",
        "outputId": "64a2e454-dd12-4ae6-834a-cd856ea0971e"
      },
      "source": [
        "tokenizer.save_model(\".\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vocab.json', './merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHbGAlyODPaB"
      },
      "source": [
        "tokenizer = SentencePieceBPETokenizer.from_file(vocab_filename=\"vocab.json\", merges_filename=\"merges.txt\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p63PBpMZtNii",
        "outputId": "b983f891-880b-4517-92c6-df11f2062b08"
      },
      "source": [
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n",
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n",
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n",
        "print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n",
        "print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "[1005, 579, 6613, 1303, 1041, 2071, 1136, 596, 1033]\n",
            "['▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.']\n",
            "['▁<', 's', '>', '이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.', '<', '/s', '>']\n",
            "<s>이순신은 조선 중기의 무신이다.</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyf_iqsnDa9-",
        "outputId": "25ed3eb2-5e7b-4d2c-efd4-162218b97991"
      },
      "source": [
        "tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<shkim>\"])\n",
        "tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
        "tokenizer.unk_token_id = tokenizer.token_to_id(\"<unk>\")\n",
        "tokenizer.bos_token_id = tokenizer.token_to_id(\"<bos>\")\n",
        "tokenizer.eos_token_id = tokenizer.token_to_id(\"<eos>\")\n",
        "\n",
        "print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids)\n",
        "print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n",
        "print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1005, 579, 6613, 1303, 1041, 2071, 1136, 596, 1033, 2]\n",
            "['<s>', '▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.', '</s>']\n",
            "이순신은 조선 중기의 무신이다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHWcg4ba7E-U"
      },
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "# creating the configurations from which the model can be made\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer.get_vocab_size(),\n",
        "  bos_token_id=tokenizer.token_to_id(\"<s>\"),\n",
        "  eos_token_id=tokenizer.token_to_id(\"</s>\"),\n",
        ")\n",
        "# creating the model\n",
        "model = GPT2LMHeadModel(config)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhsrdEnd7Xur",
        "outputId": "9cdd288e-b03c-494a-ab1e-4b2144a5a17f"
      },
      "source": [
        "model.num_parameters()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93523200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMcqoMGE1p3A"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "from filelock import FileLock\n",
        "\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "from transformers.utils import logging\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        file_path: str,\n",
        "        block_size: int,\n",
        "        overwrite_cache=False,\n",
        "        cache_dir: Optional[str] = None,\n",
        "    ):\n",
        "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
        "\n",
        "        block_size = block_size - tokenizer.num_special_tokens_to_add(is_pair=False)\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            cache_dir if cache_dir is not None else directory,\n",
        "            \"cached_lm_{}_{}_{}\".format(\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(block_size),\n",
        "                filename,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Make sure only the first process in distributed training processes the dataset,\n",
        "        # and the others will use the cache.\n",
        "        lock_path = cached_features_file + \".lock\"\n",
        "        with FileLock(lock_path):\n",
        "\n",
        "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"rb\") as handle:\n",
        "                    self.examples = pickle.load(handle)\n",
        "                logger.info(\n",
        "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
        "                # 여기서부터 본격적으로 데이터셋을 만들기 시작합니다.\n",
        "                self.examples = []\n",
        "                text = \"\"\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    lines = f.readlines()\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        line = \"<s>\"+line+\"</s>\" # 학습 데이터 앞 뒤에 문장 구분 기호를 추가해줍니다.\n",
        "                        text += line    # 'text' 객체에 모든 학습 데이터를 다 합쳐버립니다 :-)\n",
        "                tokenized_text = tokenizer.encode(text).ids\n",
        "\n",
        "                # 모델의 최대 sequence length만큼 데이터를 잘라서 저장합니다.\n",
        "                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
        "                    self.examples.append(\n",
        "                        tokenized_text[i : i + block_size]\n",
        "                    )\n",
        "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
        "                # If your dataset is small, first you should look for a bigger one :-) and second you\n",
        "                # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"wb\") as handle:\n",
        "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                logger.info(\n",
        "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
        "                )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i) -> torch.Tensor:\n",
        "        return torch.tensor(self.examples[i], dtype=torch.long)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3mSeU-40mmG"
      },
      "source": [
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=path,\n",
        "    block_size=128,\n",
        ")\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(    # GPT는 생성모델이기 때문에 [MASK] 가 필요 없습니다 :-)\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghC6K4AX6UbY",
        "outputId": "1c8c6dc0-aeb9-435d-c336-66c761c4e775"
      },
      "source": [
        "print(dataset[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   0, 3997, 3546, 8406,  463,    4, 5481, 9528, 1798, 1890, 2297, 1262,\n",
            "        9626, 2679, 1188, 2174,    2,    0, 5709, 5481,  254, 6466,  751, 3426,\n",
            "         873, 1556,  681,  895, 1627, 9223,  588, 3621, 1010, 3303,    2,    0,\n",
            "        6466, 7418, 2305,  403, 2217, 1074,    2,    0, 1013, 1107, 3716,  647,\n",
            "        8576, 1024,  940,   92, 7323,  371,   92,  722, 9295,  706, 1651,  453,\n",
            "        3166, 1032, 1074,    2,    0, 6343, 1262, 3716, 1009, 2932, 1176,  913,\n",
            "        2037, 1171, 3227,  845,   92,  439,  974, 1486, 1017,    3, 1323, 3914,\n",
            "        2094, 1042,    2,    0, 1382, 2068, 2225, 1095,  325,  845, 1823,  507,\n",
            "           4, 1242, 7698,    2,    0, 3897, 6466, 1053, 1077,  687, 2318, 4649,\n",
            "        5204, 5672, 1013, 1759,  117, 2742, 3004,  106,  656, 2283, 9765, 1192,\n",
            "        1796, 2449, 2546, 9938, 6466, 1053, 1037,  535])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEGfg7JL7KWJ"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='model_output',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=32, # 512:32  # 128:64\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwdMy08j7u-F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d420f9c-3d53-494d-fc24-aa634b6ffa62"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 44:00, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>7.996200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>7.534700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>7.287200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>7.026900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.797700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>6.590100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>6.385400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>6.214600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>6.061800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.900200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>5.766300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>5.646600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>5.510300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>5.404400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>5.200300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>5.107700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>5.036200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>4.945900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.863900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>4.823800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>4.744200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>4.691700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>4.653300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.601200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>4.566900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>4.544300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>4.516400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>4.493900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.479700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3000, training_loss=5.556810953776042, metrics={'train_runtime': 2641.4214, 'train_samples_per_second': 1.136, 'total_flos': 6812678799360000.0, 'epoch': 50.0, 'init_mem_cpu_alloc_delta': 2800807936, 'init_mem_gpu_alloc_delta': 387416064, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 15409152, 'train_mem_gpu_alloc_delta': 1150346752, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 6587866112})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpeIS3C2ipTZ"
      },
      "source": [
        "trainer.save_model()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxg4-f3P7zh2"
      },
      "source": [
        "USE_GPU = 1\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if (torch.cuda.is_available() and USE_GPU) else 'cpu')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKGRHjzya13d",
        "outputId": "74415e5a-d6fc-4025-9ba9-9360e9143492"
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode(\"<s>이순신\", add_special_tokens=True).ids).unsqueeze(0).to('cuda')\n",
        "\n",
        "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=3)\n",
        "for generated_sequence in output_sequences:\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, skip_special_tokens=True)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GENERATED SEQUENCE : 이순신.\n",
            "GENERATED SEQUENCE : 이순신공 비읽 수도 있다.\n",
            "GENERATED SEQUENCE : 이순신 정선죽우작젖을 여러 마생리가 수 있었다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUL4-4v8IONB"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}